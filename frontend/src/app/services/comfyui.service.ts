import { Injectable, signal } from '@angular/core';
import { BattlemapStroke, AiColorPrompt } from '../model/battlemap.model';

/**
 * ComfyUI Service
 * 
 * Handles communication with a local ComfyUI instance for AI image generation.
 * The browser connects directly to ComfyUI on the local network.
 * 
 * Uses FLUX diffusion model for high-quality D&D map generation.
 * Supports regional prompting - different prompts for different colored regions.
 */

export interface ComfyUIConfig {
  host: string;
  port: number;
}

export interface GenerationResult {
  success: boolean;
  imageUrl?: string;
  imageBlob?: Blob;
  error?: string;
}

@Injectable({
  providedIn: 'root'
})
export class ComfyUIService {
  // Default ComfyUI address - can be configured
  private config: ComfyUIConfig = {
    host: 'localhost',
    port: 8188
  };

  // Reactive state
  isAvailable = signal<boolean>(false);
  isGenerating = signal<boolean>(false);
  lastError = signal<string | null>(null);

  // Custom prompt (can be overridden per battlemap)
  private customPrompt: string | null = null;

  // AI generation settings
  private aiSettings: {
    seed: number;
    steps: number;
    cfg: number;
    denoise: number; // Used as ControlNet strength
  } = {
    seed: -1, // -1 = random
    steps: 10, // LCM models work great with 10 steps
    cfg: 1.5, // LCM uses low CFG
    denoise: 0.75 // How much to regenerate when inpainting
  };

  // Resolution for generation
  private readonly generationResolution = 1024;

  // Default prompt for D&D maps (used as base before regional inpainting)
  private readonly defaultPrompt = "A detailed fantasy map for Dungeons & Dragons, top-down view. Medieval fantasy style with parchment texture background.";

  // Client ID for WebSocket
  private clientId = this.generateClientId();

  /**
   * FLUX + ControlNet Canny workflow for proper sketch-to-map generation
   * 
   * Flow:
   * 1. LoadImage -> Scale to 1024x1024
   * 2. Canny Edge Detection (extracts lines from your sketch)
   * 3. Load FLUX ControlNet Canny model
   * 4. Apply ControlNet to guide generation with your sketch structure
   * 5. Generate from empty latent (not img2img!) but GUIDED by your sketch edges
   * 
   * This means:
   * - Your sketch STRUCTURE is preserved (walls, rooms, paths)
   * - Colors/textures are completely generated by AI
   * - You can draw in any color, even just black lines!
   */
  private workflowTemplate = {
    // ============ IMAGE INPUT ============
    // Load the input sketch image
    "1": {
      "inputs": {
        "image": "", // Will be set to uploaded filename
        "upload": "image"
      },
      "class_type": "LoadImage",
      "_meta": { "title": "Load Sketch" }
    },
    
    // Scale image to 1024x1024 for FLUX
    "2": {
      "inputs": {
        "upscale_method": "lanczos",
        "width": 1024,
        "height": 1024,
        "crop": "disabled",
        "image": ["1", 0]
      },
      "class_type": "ImageScale",
      "_meta": { "title": "Scale to 1024" }
    },
    
    // ============ CANNY EDGE DETECTION ============
    // Extract edges from sketch - this is the key to proper sketch interpretation!
    "30": {
      "inputs": {
        "low_threshold": 0.1,
        "high_threshold": 0.3,
        "image": ["2", 0]
      },
      "class_type": "Canny",
      "_meta": { "title": "Canny Edge Detection" }
    },
    
    // ============ MODEL LOADERS ============
    // Load FLUX VAE
    "10": {
      "inputs": {
        "vae_name": "ae.safetensors"
      },
      "class_type": "VAELoader",
      "_meta": { "title": "Load FLUX VAE" }
    },
    
    // Load FLUX CLIP (dual clip for FLUX)
    "11": {
      "inputs": {
        "clip_name1": "clip_l.safetensors",
        "clip_name2": "t5\\t5xxl_fp8_e4m3fn.safetensors",
        "type": "flux",
        "device": "default"
      },
      "class_type": "DualCLIPLoader",
      "_meta": { "title": "Load FLUX CLIP" }
    },
    
    // Load FLUX diffusion model
    "12": {
      "inputs": {
        "unet_name": "FLUX1\\flux1-dev-fp8.safetensors",
        "weight_dtype": "default"
      },
      "class_type": "UNETLoader",
      "_meta": { "title": "Load FLUX Model" }
    },
    
    // Load ControlNet using STANDARD ComfyUI loader
    // This works with InstantX/Union format ControlNets
    "40": {
      "inputs": {
        "control_net_name": "flux-canny-controlnet-v3.safetensors"
      },
      "class_type": "ControlNetLoader",
      "_meta": { "title": "Load ControlNet" }
    },
    
    // Apply LoRAs for D&D map style
    "51": {
      "inputs": {
        "PowerLoraLoaderHeaderWidget": { "type": "PowerLoraLoaderHeaderWidget" },
        "lora_1": {
          "on": true,
          "lora": "dnd-maps.safetensors",
          "strength": 1
        },
        "lora_2": {
          "on": true,
          "lora": "flux\\aidmaFLUXPro1.1-FLUX-v0.3.safetensors",
          "strength": 0.8
        },
        "âž• Add Lora": "",
        "model": ["12", 0],
        "clip": ["11", 0]
      },
      "class_type": "Power Lora Loader (rgthree)",
      "_meta": { "title": "D&D Map LoRAs" }
    },
    
    // ============ CONDITIONING ============
    // Encode the prompt
    "6": {
      "inputs": {
        "text": "", // Will be set to prompt
        "clip": ["51", 1]
      },
      "class_type": "CLIPTextEncode",
      "_meta": { "title": "Prompt" }
    },
    
    // Apply ControlNet using STANDARD ComfyUI node
    // This takes conditioning and returns modified conditioning
    "41": {
      "inputs": {
        "strength": 0.7, // How strongly to follow the sketch lines
        "conditioning": ["6", 0], // Text conditioning
        "control_net": ["40", 0], // ControlNet model
        "image": ["30", 0] // Canny edges
      },
      "class_type": "ControlNetApply",
      "_meta": { "title": "Apply ControlNet" }
    },
    
    // ============ LATENT SPACE ============
    // Empty latent - we're NOT doing img2img, we're generating fresh with ControlNet guidance
    "5": {
      "inputs": {
        "width": 1024,
        "height": 1024,
        "batch_size": 1
      },
      "class_type": "EmptyLatentImage",
      "_meta": { "title": "Empty Latent" }
    },
    
    // ============ SAMPLING ============
    // Random noise generator
    "25": {
      "inputs": {
        "noise_seed": 0 // Will be randomized
      },
      "class_type": "RandomNoise",
      "_meta": { "title": "Random Noise" }
    },
    
    // Sampler selection
    "16": {
      "inputs": {
        "sampler_name": "euler"
      },
      "class_type": "KSamplerSelect",
      "_meta": { "title": "Sampler" }
    },
    
    // Scheduler - denoise 1.0 since we start from empty latent
    "17": {
      "inputs": {
        "scheduler": "simple",
        "steps": 20,
        "denoise": 1.0,
        "model": ["51", 0]
      },
      "class_type": "BasicScheduler",
      "_meta": { "title": "Scheduler" }
    },
    
    // Guider connects model with ControlNet-enhanced conditioning
    "22": {
      "inputs": {
        "model": ["51", 0],
        "conditioning": ["41", 0] // ControlNet-modified conditioning
      },
      "class_type": "BasicGuider",
      "_meta": { "title": "Guider" }
    },
    
    // Advanced sampler for FLUX
    "13": {
      "inputs": {
        "noise": ["25", 0],
        "guider": ["22", 0],
        "sampler": ["16", 0],
        "sigmas": ["17", 0],
        "latent_image": ["5", 0] // Start from EMPTY, ControlNet provides structure
      },
      "class_type": "SamplerCustomAdvanced",
      "_meta": { "title": "FLUX Sampler" }
    },
    
    // ============ OUTPUT ============
    // Decode latent back to image
    "8": {
      "inputs": {
        "samples": ["13", 0],
        "vae": ["10", 0]
      },
      "class_type": "VAEDecode",
      "_meta": { "title": "Decode Image" }
    },
    
    // Preview/output
    "14": {
      "inputs": {
        "images": ["8", 0]
      },
      "class_type": "PreviewImage",
      "_meta": { "title": "Output" }
    }
  };

  private get baseUrl(): string {
    return `http://${this.config.host}:${this.config.port}`;
  }

  /**
   * Configure the ComfyUI connection
   */
  configure(config: Partial<ComfyUIConfig>): void {
    this.config = { ...this.config, ...config };
    this.checkAvailability();
  }

  /**
   * Check if ComfyUI is reachable
   */
  async checkAvailability(): Promise<boolean> {
    try {
      console.log('[ComfyUI] Checking availability at:', this.baseUrl);
      
      const response = await fetch(`${this.baseUrl}/system_stats`, {
        method: 'GET',
        mode: 'cors',
        signal: AbortSignal.timeout(5000) // 5 second timeout
      });
      
      const available = response.ok;
      this.isAvailable.set(available);
      this.lastError.set(null);
      console.log('[ComfyUI] Available:', available);
      return available;
    } catch (error: any) {
      console.error('[ComfyUI] Connection error:', error?.message || error);
      this.isAvailable.set(false);
      
      // Provide more helpful error messages
      if (error?.message?.includes('BLOCKED')) {
        this.lastError.set('Request blocked - try disabling ad blocker for localhost');
      } else if (error?.message?.includes('NetworkError') || error?.message?.includes('Failed to fetch')) {
        this.lastError.set('ComfyUI not reachable - is it running with --enable-cors-header?');
      } else {
        this.lastError.set(`ComfyUI error: ${error?.message || 'Unknown'}`);
      }
      return false;
    }
  }

  /**
   * Generate an image from a canvas drawing
   * @param canvas The canvas element with the drawing
   * @returns The generated image as a blob
   */
  async generateFromCanvas(canvas: HTMLCanvasElement): Promise<GenerationResult> {
    if (!this.isAvailable()) {
      const available = await this.checkAvailability();
      if (!available) {
        return { success: false, error: 'ComfyUI is not available' };
      }
    }

    this.isGenerating.set(true);
    this.lastError.set(null);

    try {
      // Step 1: Convert canvas to blob
      const blob = await this.canvasToBlob(canvas);
      
      // Step 2: Upload image to ComfyUI
      const filename = await this.uploadImage(blob);
      
      // Step 3: Create workflow with the uploaded image
      const workflow = this.createWorkflow(filename);
      
      // Step 4: Queue the prompt and wait for result
      const result = await this.queueAndWait(workflow);
      
      this.isGenerating.set(false);
      return result;
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Unknown error';
      this.lastError.set(errorMessage);
      this.isGenerating.set(false);
      return { success: false, error: errorMessage };
    }
  }

  /**
   * Generate an image from AI strokes with regional prompting
   * Each color in the AI strokes gets its own prompt from the color-prompt mappings.
   * Uses multi-pass inpainting for precise regional control:
   * 1. Generate base map from combined prompt
   * 2. For each colored region, inpaint with that region's specific prompt
   * 
   * This gives NVIDIA Canvas-style control - forest appears exactly where you paint green, etc.
   * 
   * @param aiStrokes The AI drawing strokes
   * @param colorPrompts Color-to-prompt mappings
   * @param canvasWidth Width of the source canvas
   * @param canvasHeight Height of the source canvas
   * @param bounds World coordinate bounds for rendering strokes
   */
  async generateFromAiStrokes(
    aiStrokes: BattlemapStroke[],
    colorPrompts: AiColorPrompt[],
    canvasWidth: number,
    canvasHeight: number,
    bounds: { panX: number; panY: number; scale: number }
  ): Promise<GenerationResult> {
    if (!this.isAvailable()) {
      const available = await this.checkAvailability();
      if (!available) {
        return { success: false, error: 'ComfyUI is not available' };
      }
    }

    if (aiStrokes.length === 0) {
      return { success: false, error: 'No AI strokes to generate from' };
    }

    this.isGenerating.set(true);
    this.lastError.set(null);

    try {
      // Step 1: Identify unique colors used in strokes
      const usedColors = this.getUsedColors(aiStrokes, colorPrompts);
      console.log('[ComfyUI] Multi-pass inpainting with regions:', usedColors.map(c => `${c.name}: ${c.prompt}`));

      // Step 2: Render strokes at generation resolution
      const strokeCanvas = this.renderAiStrokesToCanvas(
        aiStrokes, 
        this.generationResolution, 
        this.generationResolution, 
        {
          panX: bounds.panX * (this.generationResolution / canvasWidth),
          panY: bounds.panY * (this.generationResolution / canvasHeight),
          scale: bounds.scale * (this.generationResolution / canvasWidth)
        }
      );
      
      // Step 3: Upload the stroke canvas as base image
      const strokeBlob = await this.canvasToBlob(strokeCanvas);
      const baseImageFilename = await this.uploadImage(strokeBlob);
      console.log('[ComfyUI] Uploaded base image:', baseImageFilename);

      // Step 4: Create masks for each color region
      const regionMasks: { color: string; prompt: string; maskFilename: string }[] = [];
      for (const colorPrompt of usedColors) {
        const maskBlob = await this.createColorMask(strokeCanvas, colorPrompt.color);
        const maskFilename = await this.uploadImage(maskBlob);
        regionMasks.push({
          color: colorPrompt.color,
          prompt: colorPrompt.prompt,
          maskFilename
        });
        console.log(`[ComfyUI] Uploaded mask for "${colorPrompt.name}"`);
      }

      // Step 5: Create and execute multi-pass inpainting workflow
      const workflow = this.createInpaintingWorkflow(baseImageFilename, regionMasks);
      
      // Step 5: Queue and wait for result
      const result = await this.queueAndWait(workflow);
      
      this.isGenerating.set(false);
      return result;
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Unknown error';
      console.error('[ComfyUI] Regional generation error:', error);
      this.lastError.set(errorMessage);
      this.isGenerating.set(false);
      return { success: false, error: errorMessage };
    }
  }

  /**
   * Create a binary mask for a specific color
   * White = where this color is (will be inpainted), Black = keep as is
   */
  private async createColorMask(
    sourceCanvas: HTMLCanvasElement,
    targetColor: string
  ): Promise<Blob> {
    const width = sourceCanvas.width;
    const height = sourceCanvas.height;
    
    const maskCanvas = document.createElement('canvas');
    maskCanvas.width = width;
    maskCanvas.height = height;
    const ctx = maskCanvas.getContext('2d', { willReadFrequently: true })!;
    
    // Get source pixels
    const sourceCtx = sourceCanvas.getContext('2d', { willReadFrequently: true })!;
    const imageData = sourceCtx.getImageData(0, 0, width, height);
    const pixels = imageData.data;
    
    // Parse target color
    const targetRgb = this.hexToRgb(targetColor);
    if (!targetRgb) {
      // Fallback: return black mask (no inpainting)
      ctx.fillStyle = '#000000';
      ctx.fillRect(0, 0, width, height);
      return this.canvasToBlob(maskCanvas);
    }
    
    // Create mask - black background (preserve), white where color matches (inpaint)
    ctx.fillStyle = '#000000';
    ctx.fillRect(0, 0, width, height);
    
    const maskData = ctx.getImageData(0, 0, width, height);
    const maskPixels = maskData.data;
    
    // Color tolerance for matching
    const tolerance = 50;
    
    for (let i = 0; i < pixels.length; i += 4) {
      const r = pixels[i];
      const g = pixels[i + 1];
      const b = pixels[i + 2];
      
      if (
        Math.abs(r - targetRgb.r) <= tolerance &&
        Math.abs(g - targetRgb.g) <= tolerance &&
        Math.abs(b - targetRgb.b) <= tolerance
      ) {
        // White = inpaint this area
        maskPixels[i] = 255;
        maskPixels[i + 1] = 255;
        maskPixels[i + 2] = 255;
        maskPixels[i + 3] = 255;
      }
    }
    
    ctx.putImageData(maskData, 0, 0);
    return this.canvasToBlob(maskCanvas);
  }

  /**
   * Convert hex color to RGB
   */
  private hexToRgb(hex: string): { r: number; g: number; b: number } | null {
    const result = /^#?([a-f\d]{2})([a-f\d]{2})([a-f\d]{2})$/i.exec(hex);
    return result ? {
      r: parseInt(result[1], 16),
      g: parseInt(result[2], 16),
      b: parseInt(result[3], 16)
    } : null;
  }

  /**
   * Render AI strokes to an offscreen canvas
   */
  private renderAiStrokesToCanvas(
    strokes: BattlemapStroke[],
    width: number,
    height: number,
    bounds: { panX: number; panY: number; scale: number }
  ): HTMLCanvasElement {
    const canvas = document.createElement('canvas');
    canvas.width = width;
    canvas.height = height;
    const ctx = canvas.getContext('2d')!;
    
    // White background
    ctx.fillStyle = '#ffffff';
    ctx.fillRect(0, 0, width, height);
    
    // Apply transforms
    ctx.translate(bounds.panX, bounds.panY);
    ctx.scale(bounds.scale, bounds.scale);
    
    // Render all strokes
    for (const stroke of strokes) {
      if (stroke.points.length < 2) continue;
      
      ctx.beginPath();
      ctx.moveTo(stroke.points[0].x, stroke.points[0].y);
      
      for (let i = 1; i < stroke.points.length; i++) {
        ctx.lineTo(stroke.points[i].x, stroke.points[i].y);
      }
      
      ctx.strokeStyle = stroke.color;
      ctx.lineWidth = stroke.lineWidth;
      ctx.lineCap = 'round';
      ctx.lineJoin = 'round';
      ctx.stroke();
    }
    
    return canvas;
  }

  /**
   * Get the colors actually used in the strokes
   */
  private getUsedColors(strokes: BattlemapStroke[], colorPrompts: AiColorPrompt[]): AiColorPrompt[] {
    const usedColorSet = new Set<string>();
    
    for (const stroke of strokes) {
      usedColorSet.add(stroke.color.toLowerCase());
    }
    
    // Match to color prompts (normalize hex colors)
    return colorPrompts.filter(cp => 
      usedColorSet.has(cp.color.toLowerCase())
    );
  }

  /**
   * Create a workflow with combined prompts for all regions
   * 
   * This simplified workflow:
   * 1. Uses the colored sketch as ControlNet input (structure guide)
   * 2. Combines all region prompts into one descriptive prompt
   * 3. FLUX generates based on structure + combined description
   * 
   * Note: True per-region prompting with masks doesn't work well with FLUX.
   * The colored regions serve as visual guide via ControlNet instead.
   */
  /**
   * Create an inpainting workflow using SDXL LCM model
   * Multi-pass approach: generate base, then inpaint each region
   */
  private createInpaintingWorkflow(
    baseImageFilename: string,
    regions: { color: string; prompt: string; maskFilename: string }[]
  ): any {
    const workflow: any = {};
    let nodeId = 1;

    // ============ MODEL LOADERS ============
    // Load SDXL LCM checkpoint
    workflow[String(nodeId++)] = {
      "inputs": { "ckpt_name": "hephaistosNextgenxlLCM_v20.safetensors" },
      "class_type": "CheckpointLoaderSimple",
      "_meta": { "title": "Load SDXL LCM" }
    };
    const checkpointNode = nodeId - 1;

    // Load D&D Maps LoRA
    workflow[String(nodeId++)] = {
      "inputs": {
        "lora_name": "dnd-maps.safetensors",
        "strength_model": 1.0,
        "strength_clip": 1.0,
        "model": [String(checkpointNode), 0],
        "clip": [String(checkpointNode), 1]
      },
      "class_type": "LoraLoader",
      "_meta": { "title": "D&D Maps LoRA" }
    };
    const loraNode = nodeId - 1;

    // ============ BASE IMAGE ============
    // Load the colored sketch as base
    workflow[String(nodeId++)] = {
      "inputs": { "image": baseImageFilename, "upload": "image" },
      "class_type": "LoadImage",
      "_meta": { "title": "Load Base Image" }
    };
    const loadImageNode = nodeId - 1;

    // Scale to generation resolution
    workflow[String(nodeId++)] = {
      "inputs": {
        "upscale_method": "lanczos",
        "width": this.generationResolution,
        "height": this.generationResolution,
        "crop": "disabled",
        "image": [String(loadImageNode), 0]
      },
      "class_type": "ImageScale",
      "_meta": { "title": "Scale Image" }
    };
    const scaledImageNode = nodeId - 1;

    // ============ BASE GENERATION ============
    // Create combined base prompt
    const basePrompt = this.customPrompt || this.defaultPrompt;
    const regionDescriptions = regions.map(r => r.prompt).join(', ');
    const combinedBasePrompt = `${basePrompt}, featuring: ${regionDescriptions}`;

    // Encode base prompt
    workflow[String(nodeId++)] = {
      "inputs": {
        "text": combinedBasePrompt,
        "clip": [String(loraNode), 1]
      },
      "class_type": "CLIPTextEncode",
      "_meta": { "title": "Base Prompt" }
    };
    const baseCondNode = nodeId - 1;

    // Negative prompt
    workflow[String(nodeId++)] = {
      "inputs": {
        "text": "blurry, low quality, distorted, text, watermark, ugly",
        "clip": [String(loraNode), 1]
      },
      "class_type": "CLIPTextEncode",
      "_meta": { "title": "Negative" }
    };
    const negativeNode = nodeId - 1;

    // Encode base image to latent
    workflow[String(nodeId++)] = {
      "inputs": {
        "pixels": [String(scaledImageNode), 0],
        "vae": [String(checkpointNode), 2]
      },
      "class_type": "VAEEncode",
      "_meta": { "title": "Encode Base" }
    };
    const baseLatentNode = nodeId - 1;

    // Generate seed
    const seed = this.aiSettings.seed === -1 
      ? Math.floor(Math.random() * Number.MAX_SAFE_INTEGER)
      : this.aiSettings.seed;

    // First pass: img2img on base to establish style
    workflow[String(nodeId++)] = {
      "inputs": {
        "seed": seed,
        "steps": this.aiSettings.steps,
        "cfg": this.aiSettings.cfg,
        "sampler_name": "euler_ancestral",
        "scheduler": "normal",
        "denoise": 0.5, // Lower denoise for base - preserve sketch structure
        "model": [String(loraNode), 0],
        "positive": [String(baseCondNode), 0],
        "negative": [String(negativeNode), 0],
        "latent_image": [String(baseLatentNode), 0]
      },
      "class_type": "KSampler",
      "_meta": { "title": "Base Sampler" }
    };
    let currentLatentNode = nodeId - 1;

    // ============ REGIONAL INPAINTING PASSES ============
    // For each region, load mask and inpaint with region-specific prompt
    for (let i = 0; i < regions.length; i++) {
      const region = regions[i];
      
      // Load region mask
      workflow[String(nodeId++)] = {
        "inputs": { "image": region.maskFilename, "upload": "image" },
        "class_type": "LoadImage",
        "_meta": { "title": `Load Mask: ${region.prompt}` }
      };
      const maskImageNode = nodeId - 1;

      // Convert image to mask (use red channel)
      workflow[String(nodeId++)] = {
        "inputs": {
          "channel": "red",
          "image": [String(maskImageNode), 0]
        },
        "class_type": "ImageToMask",
        "_meta": { "title": `Convert Mask: ${region.prompt}` }
      };
      const maskNode = nodeId - 1;

      // Apply mask to latent for inpainting
      workflow[String(nodeId++)] = {
        "inputs": {
          "samples": [String(currentLatentNode), 0],
          "mask": [String(maskNode), 0]
        },
        "class_type": "SetLatentNoiseMask",
        "_meta": { "title": `Set Mask: ${region.prompt}` }
      };
      const maskedLatentNode = nodeId - 1;

      // Encode region-specific prompt
      const regionPrompt = `${basePrompt}, ${region.prompt}, detailed, high quality`;
      workflow[String(nodeId++)] = {
        "inputs": {
          "text": regionPrompt,
          "clip": [String(loraNode), 1]
        },
        "class_type": "CLIPTextEncode",
        "_meta": { "title": `Prompt: ${region.prompt}` }
      };
      const regionCondNode = nodeId - 1;

      // Inpaint this region
      workflow[String(nodeId++)] = {
        "inputs": {
          "seed": seed + i + 1, // Vary seed slightly per region
          "steps": this.aiSettings.steps,
          "cfg": this.aiSettings.cfg,
          "sampler_name": "euler_ancestral",
          "scheduler": "normal",
          "denoise": this.aiSettings.denoise, // Higher denoise for regions to replace content
          "model": [String(loraNode), 0],
          "positive": [String(regionCondNode), 0],
          "negative": [String(negativeNode), 0],
          "latent_image": [String(maskedLatentNode), 0]
        },
        "class_type": "KSampler",
        "_meta": { "title": `Inpaint: ${region.prompt}` }
      };
      currentLatentNode = nodeId - 1;
    }

    // ============ OUTPUT ============
    // VAE Decode final result
    workflow[String(nodeId++)] = {
      "inputs": {
        "samples": [String(currentLatentNode), 0],
        "vae": [String(checkpointNode), 2]
      },
      "class_type": "VAEDecode",
      "_meta": { "title": "Decode Final" }
    };
    const decodeNode = nodeId - 1;

    // Preview output
    workflow[String(nodeId++)] = {
      "inputs": { "images": [String(decodeNode), 0] },
      "class_type": "PreviewImage",
      "_meta": { "title": "Output" }
    };
    this.lastOutputNode = nodeId - 1;

    return workflow;
  }

  // Track the output node for result fetching
  private lastOutputNode = 14; // Default for standard workflow

  /**
   * Convert canvas to PNG blob
   */
  private canvasToBlob(canvas: HTMLCanvasElement): Promise<Blob> {
    return new Promise((resolve, reject) => {
      canvas.toBlob((blob) => {
        if (blob) {
          resolve(blob);
        } else {
          reject(new Error('Failed to convert canvas to blob'));
        }
      }, 'image/png');
    });
  }

  /**
   * Upload an image to ComfyUI
   */
  private async uploadImage(blob: Blob): Promise<string> {
    const formData = new FormData();
    const filename = `battlemap_input_${Date.now()}.png`;
    formData.append('image', blob, filename);
    formData.append('overwrite', 'true');

    const response = await fetch(`${this.baseUrl}/upload/image`, {
      method: 'POST',
      body: formData
    });

    if (!response.ok) {
      throw new Error(`Failed to upload image: ${response.statusText}`);
    }

    const result = await response.json();
    return result.name; // The filename on the server
  }

  /**
   * Set a custom prompt for AI generation
   */
  setCustomPrompt(prompt: string | null): void {
    this.customPrompt = prompt && prompt.trim() ? prompt : null;
  }

  /**
   * Set AI generation settings
   */
  setSettings(settings: { seed?: number; steps?: number; cfg?: number; denoise?: number }): void {
    if (settings.seed !== undefined) this.aiSettings.seed = settings.seed;
    if (settings.steps !== undefined) this.aiSettings.steps = settings.steps;
    if (settings.cfg !== undefined) this.aiSettings.cfg = settings.cfg;
    if (settings.denoise !== undefined) this.aiSettings.denoise = settings.denoise;
  }

  /**
   * Get current settings
   */
  getSettings(): { seed: number; steps: number; cfg: number; denoise: number } {
    return { ...this.aiSettings };
  }

  /**
   * Get the current prompt (custom or default)
   */
  getCurrentPrompt(): string {
    return this.customPrompt || this.defaultPrompt;
  }

  /**
   * Create a workflow with the specified input image
   * Updated for FLUX workflow structure
   */
  private createWorkflow(inputImageFilename: string): any {
    const workflow = JSON.parse(JSON.stringify(this.workflowTemplate));
    
    // Set the input image (node 1 = LoadImage)
    workflow["1"].inputs.image = inputImageFilename;
    
    // Set seed - use random if -1, otherwise use the specified seed
    // Node 25 = RandomNoise
    const seed = this.aiSettings.seed === -1 
      ? Math.floor(Math.random() * Number.MAX_SAFE_INTEGER)
      : this.aiSettings.seed;
    workflow["25"].inputs.noise_seed = seed;
    
    // Set steps on the scheduler (node 17 = BasicScheduler)
    workflow["17"].inputs.steps = this.aiSettings.steps;
    // Denoise is always 1.0 for ControlNet (full generation, not img2img)
    workflow["17"].inputs.denoise = 1.0;
    
    // Use the "denoise" setting as ControlNet strength instead
    // Node 41 = ControlNetApplyAdvanced
    // Higher value = follows sketch more closely
    workflow["41"].inputs.strength = this.aiSettings.denoise;
    
    // Set the prompt (node 6 = CLIPTextEncode)
    const prompt = this.customPrompt || this.defaultPrompt;
    workflow["6"].inputs.text = prompt;
    
    return workflow;
  }

  /**
   * Queue a prompt and wait for the result using WebSocket
   */
  private async queueAndWait(workflow: any): Promise<GenerationResult> {
    // Queue the prompt
    console.log('[ComfyUI] Queueing workflow...');
    const queueResponse = await fetch(`${this.baseUrl}/prompt`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        prompt: workflow,
        client_id: this.clientId
      })
    });

    if (!queueResponse.ok) {
      // Try to get more error details
      let errorDetails = queueResponse.statusText;
      try {
        const errorJson = await queueResponse.json();
        console.error('[ComfyUI] Queue error details:', errorJson);
        if (errorJson.error) {
          errorDetails = errorJson.error.message || JSON.stringify(errorJson.error);
        }
        if (errorJson.node_errors) {
          const nodeErrors = Object.entries(errorJson.node_errors)
            .map(([node, err]: [string, any]) => `Node ${node}: ${err.class_type} - ${err.errors?.map((e: any) => e.message).join(', ')}`)
            .join('; ');
          errorDetails += ` | ${nodeErrors}`;
        }
      } catch (e) {
        // Couldn't parse error as JSON
      }
      throw new Error(`Failed to queue prompt: ${errorDetails}`);
    }

    const { prompt_id } = await queueResponse.json();
    console.log('[ComfyUI] Queued prompt:', prompt_id);

    // Wait for completion via WebSocket
    return this.waitForCompletion(prompt_id);
  }

  /**
   * Wait for prompt completion using WebSocket
   */
  private waitForCompletion(promptId: string): Promise<GenerationResult> {
    return new Promise((resolve, reject) => {
      const wsUrl = `ws://${this.config.host}:${this.config.port}/ws?clientId=${this.clientId}`;
      const ws = new WebSocket(wsUrl);
      
      // Longer timeout for complex regional workflows (5 minutes)
      const timeout = setTimeout(() => {
        ws.close();
        reject(new Error('Generation timed out (5 min) - check ComfyUI for errors'));
      }, 300000);

      ws.onmessage = async (event) => {
        try {
          const message = JSON.parse(event.data);
          
          // Log progress for debugging
          if (message.type === 'executing' && message.data.prompt_id === promptId) {
            console.log('[ComfyUI] Executing node:', message.data.node || 'complete');
          }
          if (message.type === 'progress') {
            console.log('[ComfyUI] Progress:', message.data.value, '/', message.data.max);
          }
          
          if (message.type === 'executing' && message.data.node === null && message.data.prompt_id === promptId) {
            // Execution complete
            clearTimeout(timeout);
            ws.close();
            
            // Fetch the result
            const result = await this.fetchResult(promptId);
            resolve(result);
          }
        } catch (e) {
          // Ignore parse errors for binary messages
        }
      };

      ws.onerror = (error) => {
        clearTimeout(timeout);
        ws.close();
        reject(new Error('WebSocket error'));
      };

      ws.onclose = () => {
        clearTimeout(timeout);
      };
    });
  }

  /**
   * Fetch the generated image from history
   */
  private async fetchResult(promptId: string): Promise<GenerationResult> {
    const historyResponse = await fetch(`${this.baseUrl}/history/${promptId}`);
    
    if (!historyResponse.ok) {
      throw new Error('Failed to fetch history');
    }

    const history = await historyResponse.json();
    const outputs = history[promptId]?.outputs;

    if (!outputs) {
      throw new Error('No outputs found');
    }

    // Find the preview image output - use dynamic node ID for regional workflow
    // Try the lastOutputNode first, then fall back to common node IDs
    let previewOutput = outputs[String(this.lastOutputNode)];
    if (!previewOutput?.images?.[0]) {
      // Try other common output node IDs
      for (const nodeId of ["14", Object.keys(outputs).find(k => outputs[k]?.images?.length > 0)]) {
        if (nodeId && outputs[nodeId]?.images?.[0]) {
          previewOutput = outputs[nodeId];
          break;
        }
      }
    }
    if (!previewOutput?.images?.[0]) {
      throw new Error('No image in output');
    }

    const imageInfo = previewOutput.images[0];
    const imageUrl = `${this.baseUrl}/view?filename=${imageInfo.filename}&subfolder=${imageInfo.subfolder || ''}&type=${imageInfo.type}`;

    // Fetch the image as blob
    const imageResponse = await fetch(imageUrl);
    if (!imageResponse.ok) {
      throw new Error('Failed to fetch generated image');
    }

    const imageBlob = await imageResponse.blob();
    const blobUrl = URL.createObjectURL(imageBlob);

    return {
      success: true,
      imageUrl: blobUrl,
      imageBlob: imageBlob
    };
  }

  /**
   * Generate a unique client ID
   */
  private generateClientId(): string {
    return 'battlemap_' + Math.random().toString(36).substring(2, 15);
  }
}
