import { Injectable, signal } from '@angular/core';

/**
 * ComfyUI Service
 * 
 * Handles communication with a local ComfyUI instance for AI image generation.
 * The browser connects directly to ComfyUI on the local network.
 * 
 * Uses FLUX diffusion model for high-quality D&D map generation.
 * Workflow converts sketches to textured maps - you don't need to worry about color!
 */

export interface ComfyUIConfig {
  host: string;
  port: number;
}

export interface GenerationResult {
  success: boolean;
  imageUrl?: string;
  imageBlob?: Blob;
  error?: string;
}

@Injectable({
  providedIn: 'root'
})
export class ComfyUIService {
  // Default ComfyUI address - can be configured
  private config: ComfyUIConfig = {
    host: 'localhost',
    port: 8188
  };

  // Reactive state
  isAvailable = signal<boolean>(false);
  isGenerating = signal<boolean>(false);
  lastError = signal<string | null>(null);

  // Custom prompt (can be overridden per battlemap)
  private customPrompt: string | null = null;

  // AI generation settings
  private aiSettings: {
    seed: number;
    steps: number;
    cfg: number;
    denoise: number; // Used as ControlNet strength
  } = {
    seed: -1, // -1 = random
    steps: 25, // FLUX needs more steps
    cfg: 3.5, // Not used by FLUX, kept for compatibility
    denoise: 0.7 // ControlNet strength - how closely to follow sketch
  };

  // Default prompt for D&D maps
  private readonly defaultPrompt = "A detailed fantasy map for Dungeons & Dragons, top-down view. Medieval fantasy style with rich textures, cobblestone paths, grass, forests, and buildings. Hand-drawn RPG map aesthetic with detailed textures and atmospheric lighting.";

  // Client ID for WebSocket
  private clientId = this.generateClientId();

  /**
   * FLUX + ControlNet Canny workflow for proper sketch-to-map generation
   * 
   * Flow:
   * 1. LoadImage -> Scale to 1024x1024
   * 2. Canny Edge Detection (extracts lines from your sketch)
   * 3. Load FLUX ControlNet Canny model
   * 4. Apply ControlNet to guide generation with your sketch structure
   * 5. Generate from empty latent (not img2img!) but GUIDED by your sketch edges
   * 
   * This means:
   * - Your sketch STRUCTURE is preserved (walls, rooms, paths)
   * - Colors/textures are completely generated by AI
   * - You can draw in any color, even just black lines!
   */
  private workflowTemplate = {
    // ============ IMAGE INPUT ============
    // Load the input sketch image
    "1": {
      "inputs": {
        "image": "", // Will be set to uploaded filename
        "upload": "image"
      },
      "class_type": "LoadImage",
      "_meta": { "title": "Load Sketch" }
    },
    
    // Scale image to 1024x1024 for FLUX
    "2": {
      "inputs": {
        "upscale_method": "lanczos",
        "width": 1024,
        "height": 1024,
        "crop": "disabled",
        "image": ["1", 0]
      },
      "class_type": "ImageScale",
      "_meta": { "title": "Scale to 1024" }
    },
    
    // ============ CANNY EDGE DETECTION ============
    // Extract edges from sketch - this is the key to proper sketch interpretation!
    "30": {
      "inputs": {
        "low_threshold": 0.1,
        "high_threshold": 0.3,
        "image": ["2", 0]
      },
      "class_type": "Canny",
      "_meta": { "title": "Canny Edge Detection" }
    },
    
    // ============ MODEL LOADERS ============
    // Load FLUX VAE
    "10": {
      "inputs": {
        "vae_name": "ae.safetensors"
      },
      "class_type": "VAELoader",
      "_meta": { "title": "Load FLUX VAE" }
    },
    
    // Load FLUX CLIP (dual clip for FLUX)
    "11": {
      "inputs": {
        "clip_name1": "clip_l.safetensors",
        "clip_name2": "t5\\t5xxl_fp8_e4m3fn.safetensors",
        "type": "flux",
        "device": "default"
      },
      "class_type": "DualCLIPLoader",
      "_meta": { "title": "Load FLUX CLIP" }
    },
    
    // Load FLUX diffusion model
    "12": {
      "inputs": {
        "unet_name": "FLUX1\\flux1-dev-fp8.safetensors",
        "weight_dtype": "default"
      },
      "class_type": "UNETLoader",
      "_meta": { "title": "Load FLUX Model" }
    },
    
    // Load FLUX ControlNet Canny model (XLabs)
    // Uses LoadFluxControlNet for XLabs models
    "40": {
      "inputs": {
        "model_name": "flux-canny-controlnet-v3.safetensors"
      },
      "class_type": "LoadFluxControlNet",
      "_meta": { "title": "Load XLabs ControlNet" }
    },
    
    // Apply LoRAs for D&D map style
    "51": {
      "inputs": {
        "PowerLoraLoaderHeaderWidget": { "type": "PowerLoraLoaderHeaderWidget" },
        "lora_1": {
          "on": true,
          "lora": "dnd-maps.safetensors",
          "strength": 1
        },
        "lora_2": {
          "on": true,
          "lora": "flux\\aidmaFLUXPro1.1-FLUX-v0.3.safetensors",
          "strength": 0.8
        },
        "âž• Add Lora": "",
        "model": ["12", 0],
        "clip": ["11", 0]
      },
      "class_type": "Power Lora Loader (rgthree)",
      "_meta": { "title": "D&D Map LoRAs" }
    },
    
    // ============ CONDITIONING ============
    // Encode the prompt
    "6": {
      "inputs": {
        "text": "", // Will be set to prompt
        "clip": ["51", 1]
      },
      "class_type": "CLIPTextEncode",
      "_meta": { "title": "Prompt" }
    },
    
    // Apply XLabs ControlNet - guides generation with your sketch!
    // This modifies the model, not the conditioning
    "41": {
      "inputs": {
        "strength": 0.7, // How strongly to follow the sketch lines
        "model": ["51", 0], // Takes the LoRA'd model
        "controlnet": ["40", 0],
        "image": ["30", 0] // Use the Canny edges!
      },
      "class_type": "ApplyFluxControlNet",
      "_meta": { "title": "Apply ControlNet" }
    },
    
    // ============ LATENT SPACE ============
    // Empty latent - we're NOT doing img2img, we're generating fresh with ControlNet guidance
    "5": {
      "inputs": {
        "width": 1024,
        "height": 1024,
        "batch_size": 1
      },
      "class_type": "EmptyLatentImage",
      "_meta": { "title": "Empty Latent" }
    },
    
    // ============ SAMPLING ============
    // Random noise generator
    "25": {
      "inputs": {
        "noise_seed": 0 // Will be randomized
      },
      "class_type": "RandomNoise",
      "_meta": { "title": "Random Noise" }
    },
    
    // Sampler selection
    "16": {
      "inputs": {
        "sampler_name": "euler"
      },
      "class_type": "KSamplerSelect",
      "_meta": { "title": "Sampler" }
    },
    
    // Scheduler - denoise 1.0 since we start from empty latent
    "17": {
      "inputs": {
        "scheduler": "simple",
        "steps": 20,
        "denoise": 1.0, // Full generation, controlled by ControlNet
        "model": ["41", 0] // Use ControlNet-modified model!
      },
      "class_type": "BasicScheduler",
      "_meta": { "title": "Scheduler" }
    },
    
    // Guider connects ControlNet-modified model with conditioning
    "22": {
      "inputs": {
        "model": ["41", 0], // Use ControlNet-modified model!
        "conditioning": ["6", 0] // Regular prompt conditioning
      },
      "class_type": "BasicGuider",
      "_meta": { "title": "Guider" }
    },
    
    // Advanced sampler for FLUX
    "13": {
      "inputs": {
        "noise": ["25", 0],
        "guider": ["22", 0],
        "sampler": ["16", 0],
        "sigmas": ["17", 0],
        "latent_image": ["5", 0] // Start from EMPTY, ControlNet provides structure
      },
      "class_type": "SamplerCustomAdvanced",
      "_meta": { "title": "FLUX Sampler" }
    },
    
    // ============ OUTPUT ============
    // Decode latent back to image
    "8": {
      "inputs": {
        "samples": ["13", 0],
        "vae": ["10", 0]
      },
      "class_type": "VAEDecode",
      "_meta": { "title": "Decode Image" }
    },
    
    // Preview/output
    "14": {
      "inputs": {
        "images": ["8", 0]
      },
      "class_type": "PreviewImage",
      "_meta": { "title": "Output" }
    }
  };

  private get baseUrl(): string {
    return `http://${this.config.host}:${this.config.port}`;
  }

  /**
   * Configure the ComfyUI connection
   */
  configure(config: Partial<ComfyUIConfig>): void {
    this.config = { ...this.config, ...config };
    this.checkAvailability();
  }

  /**
   * Check if ComfyUI is reachable
   */
  async checkAvailability(): Promise<boolean> {
    try {
      console.log('[ComfyUI] Checking availability at:', this.baseUrl);
      
      const response = await fetch(`${this.baseUrl}/system_stats`, {
        method: 'GET',
        mode: 'cors',
        signal: AbortSignal.timeout(5000) // 5 second timeout
      });
      
      const available = response.ok;
      this.isAvailable.set(available);
      this.lastError.set(null);
      console.log('[ComfyUI] Available:', available);
      return available;
    } catch (error: any) {
      console.error('[ComfyUI] Connection error:', error?.message || error);
      this.isAvailable.set(false);
      
      // Provide more helpful error messages
      if (error?.message?.includes('BLOCKED')) {
        this.lastError.set('Request blocked - try disabling ad blocker for localhost');
      } else if (error?.message?.includes('NetworkError') || error?.message?.includes('Failed to fetch')) {
        this.lastError.set('ComfyUI not reachable - is it running with --enable-cors-header?');
      } else {
        this.lastError.set(`ComfyUI error: ${error?.message || 'Unknown'}`);
      }
      return false;
    }
  }

  /**
   * Generate an image from a canvas drawing
   * @param canvas The canvas element with the drawing
   * @returns The generated image as a blob
   */
  async generateFromCanvas(canvas: HTMLCanvasElement): Promise<GenerationResult> {
    if (!this.isAvailable()) {
      const available = await this.checkAvailability();
      if (!available) {
        return { success: false, error: 'ComfyUI is not available' };
      }
    }

    this.isGenerating.set(true);
    this.lastError.set(null);

    try {
      // Step 1: Convert canvas to blob
      const blob = await this.canvasToBlob(canvas);
      
      // Step 2: Upload image to ComfyUI
      const filename = await this.uploadImage(blob);
      
      // Step 3: Create workflow with the uploaded image
      const workflow = this.createWorkflow(filename);
      
      // Step 4: Queue the prompt and wait for result
      const result = await this.queueAndWait(workflow);
      
      this.isGenerating.set(false);
      return result;
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Unknown error';
      this.lastError.set(errorMessage);
      this.isGenerating.set(false);
      return { success: false, error: errorMessage };
    }
  }

  /**
   * Convert canvas to PNG blob
   */
  private canvasToBlob(canvas: HTMLCanvasElement): Promise<Blob> {
    return new Promise((resolve, reject) => {
      canvas.toBlob((blob) => {
        if (blob) {
          resolve(blob);
        } else {
          reject(new Error('Failed to convert canvas to blob'));
        }
      }, 'image/png');
    });
  }

  /**
   * Upload an image to ComfyUI
   */
  private async uploadImage(blob: Blob): Promise<string> {
    const formData = new FormData();
    const filename = `battlemap_input_${Date.now()}.png`;
    formData.append('image', blob, filename);
    formData.append('overwrite', 'true');

    const response = await fetch(`${this.baseUrl}/upload/image`, {
      method: 'POST',
      body: formData
    });

    if (!response.ok) {
      throw new Error(`Failed to upload image: ${response.statusText}`);
    }

    const result = await response.json();
    return result.name; // The filename on the server
  }

  /**
   * Set a custom prompt for AI generation
   */
  setCustomPrompt(prompt: string | null): void {
    this.customPrompt = prompt && prompt.trim() ? prompt : null;
  }

  /**
   * Set AI generation settings
   */
  setSettings(settings: { seed?: number; steps?: number; cfg?: number; denoise?: number }): void {
    if (settings.seed !== undefined) this.aiSettings.seed = settings.seed;
    if (settings.steps !== undefined) this.aiSettings.steps = settings.steps;
    if (settings.cfg !== undefined) this.aiSettings.cfg = settings.cfg;
    if (settings.denoise !== undefined) this.aiSettings.denoise = settings.denoise;
  }

  /**
   * Get current settings
   */
  getSettings(): { seed: number; steps: number; cfg: number; denoise: number } {
    return { ...this.aiSettings };
  }

  /**
   * Get the current prompt (custom or default)
   */
  getCurrentPrompt(): string {
    return this.customPrompt || this.defaultPrompt;
  }

  /**
   * Create a workflow with the specified input image
   * Updated for FLUX workflow structure
   */
  private createWorkflow(inputImageFilename: string): any {
    const workflow = JSON.parse(JSON.stringify(this.workflowTemplate));
    
    // Set the input image (node 1 = LoadImage)
    workflow["1"].inputs.image = inputImageFilename;
    
    // Set seed - use random if -1, otherwise use the specified seed
    // Node 25 = RandomNoise
    const seed = this.aiSettings.seed === -1 
      ? Math.floor(Math.random() * Number.MAX_SAFE_INTEGER)
      : this.aiSettings.seed;
    workflow["25"].inputs.noise_seed = seed;
    
    // Set steps on the scheduler (node 17 = BasicScheduler)
    workflow["17"].inputs.steps = this.aiSettings.steps;
    // Denoise is always 1.0 for ControlNet (full generation, not img2img)
    workflow["17"].inputs.denoise = 1.0;
    
    // Use the "denoise" setting as ControlNet strength instead
    // Node 41 = ControlNetApplyAdvanced
    // Higher value = follows sketch more closely
    workflow["41"].inputs.strength = this.aiSettings.denoise;
    
    // Set the prompt (node 6 = CLIPTextEncode)
    const prompt = this.customPrompt || this.defaultPrompt;
    workflow["6"].inputs.text = prompt;
    
    return workflow;
  }

  /**
   * Queue a prompt and wait for the result using WebSocket
   */
  private async queueAndWait(workflow: any): Promise<GenerationResult> {
    // Queue the prompt
    console.log('[ComfyUI] Queueing workflow...');
    const queueResponse = await fetch(`${this.baseUrl}/prompt`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        prompt: workflow,
        client_id: this.clientId
      })
    });

    if (!queueResponse.ok) {
      // Try to get more error details
      let errorDetails = queueResponse.statusText;
      try {
        const errorJson = await queueResponse.json();
        console.error('[ComfyUI] Queue error details:', errorJson);
        if (errorJson.error) {
          errorDetails = errorJson.error.message || JSON.stringify(errorJson.error);
        }
        if (errorJson.node_errors) {
          const nodeErrors = Object.entries(errorJson.node_errors)
            .map(([node, err]: [string, any]) => `Node ${node}: ${err.class_type} - ${err.errors?.map((e: any) => e.message).join(', ')}`)
            .join('; ');
          errorDetails += ` | ${nodeErrors}`;
        }
      } catch (e) {
        // Couldn't parse error as JSON
      }
      throw new Error(`Failed to queue prompt: ${errorDetails}`);
    }

    const { prompt_id } = await queueResponse.json();
    console.log('[ComfyUI] Queued prompt:', prompt_id);

    // Wait for completion via WebSocket
    return this.waitForCompletion(prompt_id);
  }

  /**
   * Wait for prompt completion using WebSocket
   */
  private waitForCompletion(promptId: string): Promise<GenerationResult> {
    return new Promise((resolve, reject) => {
      const wsUrl = `ws://${this.config.host}:${this.config.port}/ws?clientId=${this.clientId}`;
      const ws = new WebSocket(wsUrl);
      
      const timeout = setTimeout(() => {
        ws.close();
        reject(new Error('Generation timed out (60s)'));
      }, 60000);

      ws.onmessage = async (event) => {
        try {
          const message = JSON.parse(event.data);
          
          if (message.type === 'executing' && message.data.node === null && message.data.prompt_id === promptId) {
            // Execution complete
            clearTimeout(timeout);
            ws.close();
            
            // Fetch the result
            const result = await this.fetchResult(promptId);
            resolve(result);
          }
        } catch (e) {
          // Ignore parse errors for binary messages
        }
      };

      ws.onerror = (error) => {
        clearTimeout(timeout);
        ws.close();
        reject(new Error('WebSocket error'));
      };

      ws.onclose = () => {
        clearTimeout(timeout);
      };
    });
  }

  /**
   * Fetch the generated image from history
   */
  private async fetchResult(promptId: string): Promise<GenerationResult> {
    const historyResponse = await fetch(`${this.baseUrl}/history/${promptId}`);
    
    if (!historyResponse.ok) {
      throw new Error('Failed to fetch history');
    }

    const history = await historyResponse.json();
    const outputs = history[promptId]?.outputs;

    if (!outputs) {
      throw new Error('No outputs found');
    }

    // Find the preview image output (node 14)
    const previewOutput = outputs["14"];
    if (!previewOutput?.images?.[0]) {
      throw new Error('No image in output');
    }

    const imageInfo = previewOutput.images[0];
    const imageUrl = `${this.baseUrl}/view?filename=${imageInfo.filename}&subfolder=${imageInfo.subfolder || ''}&type=${imageInfo.type}`;

    // Fetch the image as blob
    const imageResponse = await fetch(imageUrl);
    if (!imageResponse.ok) {
      throw new Error('Failed to fetch generated image');
    }

    const imageBlob = await imageResponse.blob();
    const blobUrl = URL.createObjectURL(imageBlob);

    return {
      success: true,
      imageUrl: blobUrl,
      imageBlob: imageBlob
    };
  }

  /**
   * Generate a unique client ID
   */
  private generateClientId(): string {
    return 'battlemap_' + Math.random().toString(36).substring(2, 15);
  }
}
