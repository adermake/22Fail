import { Injectable, signal } from '@angular/core';
import { BattlemapStroke, AiColorPrompt } from '../model/battlemap.model';

/**
 * ComfyUI Service
 * 
 * Handles communication with a local ComfyUI instance for AI image generation.
 * The browser connects directly to ComfyUI on the local network.
 * 
 * Uses FLUX diffusion model for high-quality D&D map generation.
 * Supports regional prompting - different prompts for different colored regions.
 */

export interface ComfyUIConfig {
  host: string;
  port: number;
}

export interface GenerationResult {
  success: boolean;
  imageUrl?: string;
  imageBlob?: Blob;
  error?: string;
}

@Injectable({
  providedIn: 'root'
})
export class ComfyUIService {
  // Default ComfyUI address - can be configured
  private config: ComfyUIConfig = {
    host: 'localhost',
    port: 8188
  };

  // Reactive state
  isAvailable = signal<boolean>(false);
  isGenerating = signal<boolean>(false);
  lastError = signal<string | null>(null);

  // Custom prompt (can be overridden per battlemap)
  private customPrompt: string | null = null;

  // AI generation settings
  private aiSettings: {
    seed: number;
    steps: number;
    cfg: number;
    denoise: number; // Used as ControlNet strength
  } = {
    seed: -1, // -1 = random
    steps: 20, // FLUX can produce good results with 20 steps
    cfg: 3.5, // Not used by FLUX, kept for compatibility
    denoise: 0.7 // ControlNet strength - how closely to follow sketch
  };

  // Regional workflow uses smaller resolution for speed
  private readonly regionalResolution = 768;

  // Default prompt for D&D maps
  private readonly defaultPrompt = "A detailed fantasy map for Dungeons & Dragons, top-down view. Medieval fantasy style with rich textures, cobblestone paths, grass, forests, and buildings. Hand-drawn RPG map aesthetic with detailed textures and atmospheric lighting.";

  // Client ID for WebSocket
  private clientId = this.generateClientId();

  /**
   * FLUX + ControlNet Canny workflow for proper sketch-to-map generation
   * 
   * Flow:
   * 1. LoadImage -> Scale to 1024x1024
   * 2. Canny Edge Detection (extracts lines from your sketch)
   * 3. Load FLUX ControlNet Canny model
   * 4. Apply ControlNet to guide generation with your sketch structure
   * 5. Generate from empty latent (not img2img!) but GUIDED by your sketch edges
   * 
   * This means:
   * - Your sketch STRUCTURE is preserved (walls, rooms, paths)
   * - Colors/textures are completely generated by AI
   * - You can draw in any color, even just black lines!
   */
  private workflowTemplate = {
    // ============ IMAGE INPUT ============
    // Load the input sketch image
    "1": {
      "inputs": {
        "image": "", // Will be set to uploaded filename
        "upload": "image"
      },
      "class_type": "LoadImage",
      "_meta": { "title": "Load Sketch" }
    },
    
    // Scale image to 1024x1024 for FLUX
    "2": {
      "inputs": {
        "upscale_method": "lanczos",
        "width": 1024,
        "height": 1024,
        "crop": "disabled",
        "image": ["1", 0]
      },
      "class_type": "ImageScale",
      "_meta": { "title": "Scale to 1024" }
    },
    
    // ============ CANNY EDGE DETECTION ============
    // Extract edges from sketch - this is the key to proper sketch interpretation!
    "30": {
      "inputs": {
        "low_threshold": 0.1,
        "high_threshold": 0.3,
        "image": ["2", 0]
      },
      "class_type": "Canny",
      "_meta": { "title": "Canny Edge Detection" }
    },
    
    // ============ MODEL LOADERS ============
    // Load FLUX VAE
    "10": {
      "inputs": {
        "vae_name": "ae.safetensors"
      },
      "class_type": "VAELoader",
      "_meta": { "title": "Load FLUX VAE" }
    },
    
    // Load FLUX CLIP (dual clip for FLUX)
    "11": {
      "inputs": {
        "clip_name1": "clip_l.safetensors",
        "clip_name2": "t5\\t5xxl_fp8_e4m3fn.safetensors",
        "type": "flux",
        "device": "default"
      },
      "class_type": "DualCLIPLoader",
      "_meta": { "title": "Load FLUX CLIP" }
    },
    
    // Load FLUX diffusion model
    "12": {
      "inputs": {
        "unet_name": "FLUX1\\flux1-dev-fp8.safetensors",
        "weight_dtype": "default"
      },
      "class_type": "UNETLoader",
      "_meta": { "title": "Load FLUX Model" }
    },
    
    // Load ControlNet using STANDARD ComfyUI loader
    // This works with InstantX/Union format ControlNets
    "40": {
      "inputs": {
        "control_net_name": "flux-canny-controlnet-v3.safetensors"
      },
      "class_type": "ControlNetLoader",
      "_meta": { "title": "Load ControlNet" }
    },
    
    // Apply LoRAs for D&D map style
    "51": {
      "inputs": {
        "PowerLoraLoaderHeaderWidget": { "type": "PowerLoraLoaderHeaderWidget" },
        "lora_1": {
          "on": true,
          "lora": "dnd-maps.safetensors",
          "strength": 1
        },
        "lora_2": {
          "on": true,
          "lora": "flux\\aidmaFLUXPro1.1-FLUX-v0.3.safetensors",
          "strength": 0.8
        },
        "➕ Add Lora": "",
        "model": ["12", 0],
        "clip": ["11", 0]
      },
      "class_type": "Power Lora Loader (rgthree)",
      "_meta": { "title": "D&D Map LoRAs" }
    },
    
    // ============ CONDITIONING ============
    // Encode the prompt
    "6": {
      "inputs": {
        "text": "", // Will be set to prompt
        "clip": ["51", 1]
      },
      "class_type": "CLIPTextEncode",
      "_meta": { "title": "Prompt" }
    },
    
    // Apply ControlNet using STANDARD ComfyUI node
    // This takes conditioning and returns modified conditioning
    "41": {
      "inputs": {
        "strength": 0.7, // How strongly to follow the sketch lines
        "conditioning": ["6", 0], // Text conditioning
        "control_net": ["40", 0], // ControlNet model
        "image": ["30", 0] // Canny edges
      },
      "class_type": "ControlNetApply",
      "_meta": { "title": "Apply ControlNet" }
    },
    
    // ============ LATENT SPACE ============
    // Empty latent - we're NOT doing img2img, we're generating fresh with ControlNet guidance
    "5": {
      "inputs": {
        "width": 1024,
        "height": 1024,
        "batch_size": 1
      },
      "class_type": "EmptyLatentImage",
      "_meta": { "title": "Empty Latent" }
    },
    
    // ============ SAMPLING ============
    // Random noise generator
    "25": {
      "inputs": {
        "noise_seed": 0 // Will be randomized
      },
      "class_type": "RandomNoise",
      "_meta": { "title": "Random Noise" }
    },
    
    // Sampler selection
    "16": {
      "inputs": {
        "sampler_name": "euler"
      },
      "class_type": "KSamplerSelect",
      "_meta": { "title": "Sampler" }
    },
    
    // Scheduler - denoise 1.0 since we start from empty latent
    "17": {
      "inputs": {
        "scheduler": "simple",
        "steps": 20,
        "denoise": 1.0,
        "model": ["51", 0]
      },
      "class_type": "BasicScheduler",
      "_meta": { "title": "Scheduler" }
    },
    
    // Guider connects model with ControlNet-enhanced conditioning
    "22": {
      "inputs": {
        "model": ["51", 0],
        "conditioning": ["41", 0] // ControlNet-modified conditioning
      },
      "class_type": "BasicGuider",
      "_meta": { "title": "Guider" }
    },
    
    // Advanced sampler for FLUX
    "13": {
      "inputs": {
        "noise": ["25", 0],
        "guider": ["22", 0],
        "sampler": ["16", 0],
        "sigmas": ["17", 0],
        "latent_image": ["5", 0] // Start from EMPTY, ControlNet provides structure
      },
      "class_type": "SamplerCustomAdvanced",
      "_meta": { "title": "FLUX Sampler" }
    },
    
    // ============ OUTPUT ============
    // Decode latent back to image
    "8": {
      "inputs": {
        "samples": ["13", 0],
        "vae": ["10", 0]
      },
      "class_type": "VAEDecode",
      "_meta": { "title": "Decode Image" }
    },
    
    // Preview/output
    "14": {
      "inputs": {
        "images": ["8", 0]
      },
      "class_type": "PreviewImage",
      "_meta": { "title": "Output" }
    }
  };

  private get baseUrl(): string {
    return `http://${this.config.host}:${this.config.port}`;
  }

  /**
   * Configure the ComfyUI connection
   */
  configure(config: Partial<ComfyUIConfig>): void {
    this.config = { ...this.config, ...config };
    this.checkAvailability();
  }

  /**
   * Check if ComfyUI is reachable
   */
  async checkAvailability(): Promise<boolean> {
    try {
      console.log('[ComfyUI] Checking availability at:', this.baseUrl);
      
      const response = await fetch(`${this.baseUrl}/system_stats`, {
        method: 'GET',
        mode: 'cors',
        signal: AbortSignal.timeout(5000) // 5 second timeout
      });
      
      const available = response.ok;
      this.isAvailable.set(available);
      this.lastError.set(null);
      console.log('[ComfyUI] Available:', available);
      return available;
    } catch (error: any) {
      console.error('[ComfyUI] Connection error:', error?.message || error);
      this.isAvailable.set(false);
      
      // Provide more helpful error messages
      if (error?.message?.includes('BLOCKED')) {
        this.lastError.set('Request blocked - try disabling ad blocker for localhost');
      } else if (error?.message?.includes('NetworkError') || error?.message?.includes('Failed to fetch')) {
        this.lastError.set('ComfyUI not reachable - is it running with --enable-cors-header?');
      } else {
        this.lastError.set(`ComfyUI error: ${error?.message || 'Unknown'}`);
      }
      return false;
    }
  }

  /**
   * Generate an image from a canvas drawing
   * @param canvas The canvas element with the drawing
   * @returns The generated image as a blob
   */
  async generateFromCanvas(canvas: HTMLCanvasElement): Promise<GenerationResult> {
    if (!this.isAvailable()) {
      const available = await this.checkAvailability();
      if (!available) {
        return { success: false, error: 'ComfyUI is not available' };
      }
    }

    this.isGenerating.set(true);
    this.lastError.set(null);

    try {
      // Step 1: Convert canvas to blob
      const blob = await this.canvasToBlob(canvas);
      
      // Step 2: Upload image to ComfyUI
      const filename = await this.uploadImage(blob);
      
      // Step 3: Create workflow with the uploaded image
      const workflow = this.createWorkflow(filename);
      
      // Step 4: Queue the prompt and wait for result
      const result = await this.queueAndWait(workflow);
      
      this.isGenerating.set(false);
      return result;
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Unknown error';
      this.lastError.set(errorMessage);
      this.isGenerating.set(false);
      return { success: false, error: errorMessage };
    }
  }

  /**
   * Generate an image from AI strokes with regional prompting
   * Each color in the AI strokes gets its own prompt from the color-prompt mappings
   * 
   * @param aiStrokes The AI drawing strokes
   * @param colorPrompts Color-to-prompt mappings
   * @param canvasWidth Width of the source canvas
   * @param canvasHeight Height of the source canvas
   * @param bounds World coordinate bounds for rendering strokes
   */
  async generateFromAiStrokes(
    aiStrokes: BattlemapStroke[],
    colorPrompts: AiColorPrompt[],
    canvasWidth: number,
    canvasHeight: number,
    bounds: { panX: number; panY: number; scale: number }
  ): Promise<GenerationResult> {
    if (!this.isAvailable()) {
      const available = await this.checkAvailability();
      if (!available) {
        return { success: false, error: 'ComfyUI is not available' };
      }
    }

    if (aiStrokes.length === 0) {
      return { success: false, error: 'No AI strokes to generate from' };
    }

    this.isGenerating.set(true);
    this.lastError.set(null);

    try {
      // Step 1: Render AI strokes to a SMALL canvas for mask detection
      // Step 1: Identify unique colors used in strokes to build combined prompt
      const usedColors = this.getUsedColors(aiStrokes, colorPrompts);
      console.log('[ComfyUI] Generating with region prompts:', usedColors.map(c => `${c.name}: ${c.prompt}`));

      // Step 2: Render strokes at generation resolution for ControlNet
      // The colored sketch provides structure guidance via edge detection
      const strokeCanvas = this.renderAiStrokesToCanvas(
        aiStrokes, 
        this.regionalResolution, 
        this.regionalResolution, 
        {
          panX: bounds.panX * (this.regionalResolution / canvasWidth),
          panY: bounds.panY * (this.regionalResolution / canvasHeight),
          scale: bounds.scale * (this.regionalResolution / canvasWidth)
        }
      );
      
      // Step 3: Upload sketch image for ControlNet
      const mainImageFilename = await this.uploadImage(await this.canvasToBlob(strokeCanvas));

      // Step 4: Create simplified workflow (combined prompt + ControlNet)
      // We pass the region info for building the combined prompt
      const regions = usedColors.map(c => ({ color: c.color, prompt: c.prompt, filename: '' }));
      const workflow = this.createRegionalWorkflow(mainImageFilename, regions);
      
      // Step 5: Queue and wait for result
      const result = await this.queueAndWait(workflow);
      
      this.isGenerating.set(false);
      return result;
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Unknown error';
      console.error('[ComfyUI] Regional generation error:', error);
      this.lastError.set(errorMessage);
      this.isGenerating.set(false);
      return { success: false, error: errorMessage };
    }
  }

  /**
   * Render AI strokes to an offscreen canvas
   */
  private renderAiStrokesToCanvas(
    strokes: BattlemapStroke[],
    width: number,
    height: number,
    bounds: { panX: number; panY: number; scale: number }
  ): HTMLCanvasElement {
    const canvas = document.createElement('canvas');
    canvas.width = width;
    canvas.height = height;
    const ctx = canvas.getContext('2d')!;
    
    // White background
    ctx.fillStyle = '#ffffff';
    ctx.fillRect(0, 0, width, height);
    
    // Apply transforms
    ctx.translate(bounds.panX, bounds.panY);
    ctx.scale(bounds.scale, bounds.scale);
    
    // Render all strokes
    for (const stroke of strokes) {
      if (stroke.points.length < 2) continue;
      
      ctx.beginPath();
      ctx.moveTo(stroke.points[0].x, stroke.points[0].y);
      
      for (let i = 1; i < stroke.points.length; i++) {
        ctx.lineTo(stroke.points[i].x, stroke.points[i].y);
      }
      
      ctx.strokeStyle = stroke.color;
      ctx.lineWidth = stroke.lineWidth;
      ctx.lineCap = 'round';
      ctx.lineJoin = 'round';
      ctx.stroke();
    }
    
    return canvas;
  }

  /**
   * Get the colors actually used in the strokes
   */
  private getUsedColors(strokes: BattlemapStroke[], colorPrompts: AiColorPrompt[]): AiColorPrompt[] {
    const usedColorSet = new Set<string>();
    
    for (const stroke of strokes) {
      usedColorSet.add(stroke.color.toLowerCase());
    }
    
    // Match to color prompts (normalize hex colors)
    return colorPrompts.filter(cp => 
      usedColorSet.has(cp.color.toLowerCase())
    );
  }

  /**
   * Create a workflow with combined prompts for all regions
   * 
   * This simplified workflow:
   * 1. Uses the colored sketch as ControlNet input (structure guide)
   * 2. Combines all region prompts into one descriptive prompt
   * 3. FLUX generates based on structure + combined description
   * 
   * Note: True per-region prompting with masks doesn't work well with FLUX.
   * The colored regions serve as visual guide via ControlNet instead.
   */
  private createRegionalWorkflow(
    mainImageFilename: string,
    regions: { color: string; prompt: string; filename: string }[]
  ): any {
    // Start with base workflow structure
    const workflow: any = {};
    let nodeId = 1;

    // ============ IMAGE INPUT ============
    // Load the main sketch image
    workflow[String(nodeId++)] = {
      "inputs": { "image": mainImageFilename, "upload": "image" },
      "class_type": "LoadImage",
      "_meta": { "title": "Load Sketch" }
    };
    const loadImageNode = nodeId - 1;

    // Scale to smaller size for faster regional generation
    workflow[String(nodeId++)] = {
      "inputs": {
        "upscale_method": "lanczos",
        "width": this.regionalResolution,
        "height": this.regionalResolution,
        "crop": "disabled",
        "image": [String(loadImageNode), 0]
      },
      "class_type": "ImageScale",
      "_meta": { "title": "Scale Image" }
    };
    const scaledImageNode = nodeId - 1;

    // Canny edge detection
    workflow[String(nodeId++)] = {
      "inputs": {
        "low_threshold": 0.1,
        "high_threshold": 0.3,
        "image": [String(scaledImageNode), 0]
      },
      "class_type": "Canny",
      "_meta": { "title": "Canny Edge Detection" }
    };
    const cannyNode = nodeId - 1;

    // ============ MODEL LOADERS ============
    // VAE
    workflow[String(nodeId++)] = {
      "inputs": { "vae_name": "ae.safetensors" },
      "class_type": "VAELoader",
      "_meta": { "title": "Load FLUX VAE" }
    };
    const vaeNode = nodeId - 1;

    // CLIP
    workflow[String(nodeId++)] = {
      "inputs": {
        "clip_name1": "clip_l.safetensors",
        "clip_name2": "t5\\t5xxl_fp8_e4m3fn.safetensors",
        "type": "flux",
        "device": "default"
      },
      "class_type": "DualCLIPLoader",
      "_meta": { "title": "Load FLUX CLIP" }
    };
    const clipNode = nodeId - 1;

    // UNET
    workflow[String(nodeId++)] = {
      "inputs": {
        "unet_name": "FLUX1\\flux1-dev-fp8.safetensors",
        "weight_dtype": "default"
      },
      "class_type": "UNETLoader",
      "_meta": { "title": "Load FLUX Model" }
    };
    const unetNode = nodeId - 1;

    // ControlNet
    workflow[String(nodeId++)] = {
      "inputs": { "control_net_name": "flux-canny-controlnet-v3.safetensors" },
      "class_type": "ControlNetLoader",
      "_meta": { "title": "Load ControlNet" }
    };
    const controlNetNode = nodeId - 1;

    // LoRAs
    workflow[String(nodeId++)] = {
      "inputs": {
        "PowerLoraLoaderHeaderWidget": { "type": "PowerLoraLoaderHeaderWidget" },
        "lora_1": { "on": true, "lora": "dnd-maps.safetensors", "strength": 1 },
        "lora_2": { "on": true, "lora": "flux\\aidmaFLUXPro1.1-FLUX-v0.3.safetensors", "strength": 0.8 },
        "➕ Add Lora": "",
        "model": [String(unetNode), 0],
        "clip": [String(clipNode), 0]
      },
      "class_type": "Power Lora Loader (rgthree)",
      "_meta": { "title": "D&D Map LoRAs" }
    };
    const loraNode = nodeId - 1;

    // ============ REGIONAL CONDITIONING ============
    // Build combined prompt for all regions + base map prompt
    const basePrompt = this.customPrompt || this.defaultPrompt;
    const regionDescriptions = regions.map(r => r.prompt).join(', ');
    const combinedPrompt = `${basePrompt}. Map features: ${regionDescriptions}`;

    // Main text encoding with combined prompt
    workflow[String(nodeId++)] = {
      "inputs": {
        "text": combinedPrompt,
        "clip": [String(loraNode), 1]
      },
      "class_type": "CLIPTextEncode",
      "_meta": { "title": "Combined Prompt" }
    };
    const mainCondNode = nodeId - 1;

    // Apply ControlNet to combined conditioning
    workflow[String(nodeId++)] = {
      "inputs": {
        "strength": this.aiSettings.denoise,
        "conditioning": [String(mainCondNode), 0],
        "control_net": [String(controlNetNode), 0],
        "image": [String(cannyNode), 0]
      },
      "class_type": "ControlNetApply",
      "_meta": { "title": "Apply ControlNet" }
    };
    const controlNetApplyNode = nodeId - 1;

    // Use ControlNet conditioning directly (no per-region masking - doesn't work well with FLUX)
    // The colored sketch provides structure guidance via ControlNet edges
    const finalCondNode = controlNetApplyNode;

    // ============ SAMPLING ============
    // Empty latent at regional resolution
    workflow[String(nodeId++)] = {
      "inputs": { "width": this.regionalResolution, "height": this.regionalResolution, "batch_size": 1 },
      "class_type": "EmptyLatentImage",
      "_meta": { "title": "Empty Latent" }
    };
    const latentNode = nodeId - 1;

    // Random noise
    const seed = this.aiSettings.seed === -1 
      ? Math.floor(Math.random() * Number.MAX_SAFE_INTEGER)
      : this.aiSettings.seed;
    workflow[String(nodeId++)] = {
      "inputs": { "noise_seed": seed },
      "class_type": "RandomNoise",
      "_meta": { "title": "Random Noise" }
    };
    const noiseNode = nodeId - 1;

    // Sampler
    workflow[String(nodeId++)] = {
      "inputs": { "sampler_name": "euler" },
      "class_type": "KSamplerSelect",
      "_meta": { "title": "Sampler" }
    };
    const samplerNode = nodeId - 1;

    // Scheduler
    workflow[String(nodeId++)] = {
      "inputs": {
        "scheduler": "simple",
        "steps": this.aiSettings.steps,
        "denoise": 1.0,
        "model": [String(loraNode), 0]
      },
      "class_type": "BasicScheduler",
      "_meta": { "title": "Scheduler" }
    };
    const schedulerNode = nodeId - 1;

    // Guider with combined conditioning (ControlNet applied)
    workflow[String(nodeId++)] = {
      "inputs": {
        "model": [String(loraNode), 0],
        "conditioning": [String(finalCondNode), 0]
      },
      "class_type": "BasicGuider",
      "_meta": { "title": "Guider" }
    };
    const guiderNode = nodeId - 1;

    // Advanced sampler
    workflow[String(nodeId++)] = {
      "inputs": {
        "noise": [String(noiseNode), 0],
        "guider": [String(guiderNode), 0],
        "sampler": [String(samplerNode), 0],
        "sigmas": [String(schedulerNode), 0],
        "latent_image": [String(latentNode), 0]
      },
      "class_type": "SamplerCustomAdvanced",
      "_meta": { "title": "FLUX Sampler" }
    };
    const samplerAdvNode = nodeId - 1;

    // ============ OUTPUT ============
    // VAE Decode
    workflow[String(nodeId++)] = {
      "inputs": {
        "samples": [String(samplerAdvNode), 0],
        "vae": [String(vaeNode), 0]
      },
      "class_type": "VAEDecode",
      "_meta": { "title": "Decode Image" }
    };
    const decodeNode = nodeId - 1;

    // Preview
    workflow[String(nodeId++)] = {
      "inputs": { "images": [String(decodeNode), 0] },
      "class_type": "PreviewImage",
      "_meta": { "title": "Output" }
    };
    // Store output node ID for fetching results
    this.lastOutputNode = nodeId - 1;

    return workflow;
  }

  // Track the output node for result fetching
  private lastOutputNode = 14; // Default for standard workflow

  /**
   * Convert canvas to PNG blob
   */
  private canvasToBlob(canvas: HTMLCanvasElement): Promise<Blob> {
    return new Promise((resolve, reject) => {
      canvas.toBlob((blob) => {
        if (blob) {
          resolve(blob);
        } else {
          reject(new Error('Failed to convert canvas to blob'));
        }
      }, 'image/png');
    });
  }

  /**
   * Upload an image to ComfyUI
   */
  private async uploadImage(blob: Blob): Promise<string> {
    const formData = new FormData();
    const filename = `battlemap_input_${Date.now()}.png`;
    formData.append('image', blob, filename);
    formData.append('overwrite', 'true');

    const response = await fetch(`${this.baseUrl}/upload/image`, {
      method: 'POST',
      body: formData
    });

    if (!response.ok) {
      throw new Error(`Failed to upload image: ${response.statusText}`);
    }

    const result = await response.json();
    return result.name; // The filename on the server
  }

  /**
   * Set a custom prompt for AI generation
   */
  setCustomPrompt(prompt: string | null): void {
    this.customPrompt = prompt && prompt.trim() ? prompt : null;
  }

  /**
   * Set AI generation settings
   */
  setSettings(settings: { seed?: number; steps?: number; cfg?: number; denoise?: number }): void {
    if (settings.seed !== undefined) this.aiSettings.seed = settings.seed;
    if (settings.steps !== undefined) this.aiSettings.steps = settings.steps;
    if (settings.cfg !== undefined) this.aiSettings.cfg = settings.cfg;
    if (settings.denoise !== undefined) this.aiSettings.denoise = settings.denoise;
  }

  /**
   * Get current settings
   */
  getSettings(): { seed: number; steps: number; cfg: number; denoise: number } {
    return { ...this.aiSettings };
  }

  /**
   * Get the current prompt (custom or default)
   */
  getCurrentPrompt(): string {
    return this.customPrompt || this.defaultPrompt;
  }

  /**
   * Create a workflow with the specified input image
   * Updated for FLUX workflow structure
   */
  private createWorkflow(inputImageFilename: string): any {
    const workflow = JSON.parse(JSON.stringify(this.workflowTemplate));
    
    // Set the input image (node 1 = LoadImage)
    workflow["1"].inputs.image = inputImageFilename;
    
    // Set seed - use random if -1, otherwise use the specified seed
    // Node 25 = RandomNoise
    const seed = this.aiSettings.seed === -1 
      ? Math.floor(Math.random() * Number.MAX_SAFE_INTEGER)
      : this.aiSettings.seed;
    workflow["25"].inputs.noise_seed = seed;
    
    // Set steps on the scheduler (node 17 = BasicScheduler)
    workflow["17"].inputs.steps = this.aiSettings.steps;
    // Denoise is always 1.0 for ControlNet (full generation, not img2img)
    workflow["17"].inputs.denoise = 1.0;
    
    // Use the "denoise" setting as ControlNet strength instead
    // Node 41 = ControlNetApplyAdvanced
    // Higher value = follows sketch more closely
    workflow["41"].inputs.strength = this.aiSettings.denoise;
    
    // Set the prompt (node 6 = CLIPTextEncode)
    const prompt = this.customPrompt || this.defaultPrompt;
    workflow["6"].inputs.text = prompt;
    
    return workflow;
  }

  /**
   * Queue a prompt and wait for the result using WebSocket
   */
  private async queueAndWait(workflow: any): Promise<GenerationResult> {
    // Queue the prompt
    console.log('[ComfyUI] Queueing workflow...');
    const queueResponse = await fetch(`${this.baseUrl}/prompt`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        prompt: workflow,
        client_id: this.clientId
      })
    });

    if (!queueResponse.ok) {
      // Try to get more error details
      let errorDetails = queueResponse.statusText;
      try {
        const errorJson = await queueResponse.json();
        console.error('[ComfyUI] Queue error details:', errorJson);
        if (errorJson.error) {
          errorDetails = errorJson.error.message || JSON.stringify(errorJson.error);
        }
        if (errorJson.node_errors) {
          const nodeErrors = Object.entries(errorJson.node_errors)
            .map(([node, err]: [string, any]) => `Node ${node}: ${err.class_type} - ${err.errors?.map((e: any) => e.message).join(', ')}`)
            .join('; ');
          errorDetails += ` | ${nodeErrors}`;
        }
      } catch (e) {
        // Couldn't parse error as JSON
      }
      throw new Error(`Failed to queue prompt: ${errorDetails}`);
    }

    const { prompt_id } = await queueResponse.json();
    console.log('[ComfyUI] Queued prompt:', prompt_id);

    // Wait for completion via WebSocket
    return this.waitForCompletion(prompt_id);
  }

  /**
   * Wait for prompt completion using WebSocket
   */
  private waitForCompletion(promptId: string): Promise<GenerationResult> {
    return new Promise((resolve, reject) => {
      const wsUrl = `ws://${this.config.host}:${this.config.port}/ws?clientId=${this.clientId}`;
      const ws = new WebSocket(wsUrl);
      
      // Longer timeout for complex regional workflows (5 minutes)
      const timeout = setTimeout(() => {
        ws.close();
        reject(new Error('Generation timed out (5 min) - check ComfyUI for errors'));
      }, 300000);

      ws.onmessage = async (event) => {
        try {
          const message = JSON.parse(event.data);
          
          // Log progress for debugging
          if (message.type === 'executing' && message.data.prompt_id === promptId) {
            console.log('[ComfyUI] Executing node:', message.data.node || 'complete');
          }
          if (message.type === 'progress') {
            console.log('[ComfyUI] Progress:', message.data.value, '/', message.data.max);
          }
          
          if (message.type === 'executing' && message.data.node === null && message.data.prompt_id === promptId) {
            // Execution complete
            clearTimeout(timeout);
            ws.close();
            
            // Fetch the result
            const result = await this.fetchResult(promptId);
            resolve(result);
          }
        } catch (e) {
          // Ignore parse errors for binary messages
        }
      };

      ws.onerror = (error) => {
        clearTimeout(timeout);
        ws.close();
        reject(new Error('WebSocket error'));
      };

      ws.onclose = () => {
        clearTimeout(timeout);
      };
    });
  }

  /**
   * Fetch the generated image from history
   */
  private async fetchResult(promptId: string): Promise<GenerationResult> {
    const historyResponse = await fetch(`${this.baseUrl}/history/${promptId}`);
    
    if (!historyResponse.ok) {
      throw new Error('Failed to fetch history');
    }

    const history = await historyResponse.json();
    const outputs = history[promptId]?.outputs;

    if (!outputs) {
      throw new Error('No outputs found');
    }

    // Find the preview image output - use dynamic node ID for regional workflow
    // Try the lastOutputNode first, then fall back to common node IDs
    let previewOutput = outputs[String(this.lastOutputNode)];
    if (!previewOutput?.images?.[0]) {
      // Try other common output node IDs
      for (const nodeId of ["14", Object.keys(outputs).find(k => outputs[k]?.images?.length > 0)]) {
        if (nodeId && outputs[nodeId]?.images?.[0]) {
          previewOutput = outputs[nodeId];
          break;
        }
      }
    }
    if (!previewOutput?.images?.[0]) {
      throw new Error('No image in output');
    }

    const imageInfo = previewOutput.images[0];
    const imageUrl = `${this.baseUrl}/view?filename=${imageInfo.filename}&subfolder=${imageInfo.subfolder || ''}&type=${imageInfo.type}`;

    // Fetch the image as blob
    const imageResponse = await fetch(imageUrl);
    if (!imageResponse.ok) {
      throw new Error('Failed to fetch generated image');
    }

    const imageBlob = await imageResponse.blob();
    const blobUrl = URL.createObjectURL(imageBlob);

    return {
      success: true,
      imageUrl: blobUrl,
      imageBlob: imageBlob
    };
  }

  /**
   * Generate a unique client ID
   */
  private generateClientId(): string {
    return 'battlemap_' + Math.random().toString(36).substring(2, 15);
  }
}
